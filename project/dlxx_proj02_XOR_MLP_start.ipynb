{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOm/Lqo4BOKGqAyEgCZz3aC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Redwoods/dle/blob/main/project/dlxx_proj02_XOR_MLP_start.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR problem\n",
        "## numpy with back propagation\n",
        "## MLP\n",
        "- 1-hidden layer\n",
        "- 2 hidden-layers\n",
        "- hyperparameter setting"
      ],
      "metadata": {
        "id": "w_xP4HGq3GSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRhK5urN2-WV"
      },
      "outputs": [],
      "source": [
        "# numpy - bak propagation with iterations=20000\n",
        "import numpy as np\n",
        "# XOR solution by MLP\n",
        "\n",
        "# 시그모이드 함수\n",
        "def actf(x):\n",
        "\treturn 1/(1+np.exp(-x))\n",
        "\n",
        "# 시그모이드 함수의 미분치\n",
        "def actf_deriv(x):\n",
        "\t    return x*(1-x)\n",
        "\n",
        "# 입력유닛의 개수, 은닉유닛의 개수, 출력유닛의 개수\n",
        "inputs, hiddens, outputs = 2, 2, 1\n",
        "learning_rate = 0.5\n",
        "\n",
        "# 훈련 입력과 출력\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # (4,2)\n",
        "T = np.array([[0], [1], [1], [0]])              # (4,1)\n",
        "\n",
        "# 가중치를 –1.0에서 1.0 사이의 난수로 초기화한다.\n",
        "W1 = 2*np.random.random((inputs, hiddens))-1    # (2,2)\n",
        "W2 = 2*np.random.random((hiddens, outputs))-1   # (2,1)\n",
        "B1 = np.zeros(hiddens)                          # (2,)\n",
        "B2 = np.zeros(outputs)                          # (1,)\n",
        "print(W1.shape,W2.shape,B1.shape,B2.shape)\n",
        "\n",
        "# 순방향 전파 계산\n",
        "def predict(x):\n",
        "        layer0 = x\t\t\t# 입력을 layer0에 대입한다. \n",
        "        Z1 = np.dot(layer0, W1)+B1\t# 행렬의 곱을 계산한다. \n",
        "        layer1 = actf(Z1)\t\t# 활성화 함수를 적용한다. \n",
        "        Z2 = np.dot(layer1, W2)+B2\t# 행렬의 곱을 계산한다. \n",
        "        layer2 = actf(Z2)\t\t# 활성화 함수를 적용한다. \n",
        "        return layer0, layer1, layer2\n",
        "    \n",
        "# 역방향 전파 계산\n",
        "def fit():\n",
        "    global W1, W2, B1, B2\n",
        "    for i in range(20000):\n",
        "            layer0, layer1, layer2 = predict(X) # input-batch-size = 4\n",
        "            layer2_error = layer2-T\n",
        "            # print(layer0.shape,layer1.shape,layer2.shape)\n",
        "            \n",
        "            layer2_delta = layer2_error*actf_deriv(layer2)\n",
        "            layer1_error = np.dot(layer2_delta, W2.T)\n",
        "            layer1_delta = layer1_error*actf_deriv(layer1)\n",
        "            # print(layer2_error.shape)\n",
        "            # print(layer2_delta.shape)\n",
        "            # print(layer1_error.shape)\n",
        "            # print(layer1_delta.shape)\n",
        "            \n",
        "            W2 += -learning_rate*np.dot(layer1.T, layer2_delta)/4.0  # 4개의 입력에 대한 평균 기울기\n",
        "            W1 += -learning_rate*np.dot(layer0.T, layer1_delta)/4.0\n",
        "            B2 += -learning_rate*np.sum(layer2_delta, axis=0)/4.0\n",
        "            B1 += -learning_rate*np.sum(layer1_delta, axis=0)/4.0\n",
        "\n",
        "def test():\n",
        "    for x, y in zip(X, T):\n",
        "        x = np.reshape(x, (1, -1))\t\t# 하나여도 2차원 형태이어야 한다.\n",
        "        layer0, layer1, layer2 = predict(x)\n",
        "        print(x, y, layer2)\n",
        "\n",
        "fit()\n",
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Dose numpy with back propagation solve XOR problem?"
      ],
      "metadata": {
        "id": "LLAFW892416U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras MLP with 1 hidden layer, epochs=2000\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=2, input_shape=(2,), activation='sigmoid')) #\n",
        "model.add(tf.keras.layers.Dense(units=1,  activation='sigmoid')) #\n",
        "model.compile(loss='mean_squared_error', \n",
        "              optimizer=tf.keras.optimizers.SGD(lr=0.1)) \n",
        "\n",
        "model.summary()\n",
        " \n",
        "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "model.fit(X, y, batch_size=1, epochs=2000, verbose=2)\n",
        "\n",
        "print(model.predict(X))\n",
        "\n"
      ],
      "metadata": {
        "id": "v-bAjoyT5BzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Not good result"
      ],
      "metadata": {
        "id": "Abr666XP8P33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Keras MLP with 2 hidden layers and epochs=2000\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units=16, input_shape=(2,), activation='relu')) #①\n",
        "model.add(tf.keras.layers.Dense(units=8, activation='relu')) #\n",
        "model.add(tf.keras.layers.Dense(units=1,  activation='sigmoid')) #\t\t\n",
        "model.compile(loss='mean_squared_error', \n",
        "              optimizer=tf.keras.optimizers.SGD(lr=0.1)) \n",
        "\n",
        "model.summary()\n",
        "\n",
        "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "model.fit(X, y, batch_size=1, epochs=2000, verbose=2)\n",
        "\n",
        "print(model.predict(X))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZmPMFRt38UXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "m3m0N8g__tDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [DIY] Hyperparameter control\n",
        "- Find the optimal training condition with epoches=2000\n",
        "\n",
        "> ## Try Bard, ChatGPT, Bing AI"
      ],
      "metadata": {
        "id": "VENzW81w_YlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "_ZPt5IUlBmOh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9yaoU2IBm1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}